\newcommand{\D}{D}
\chapter{\D}

For the purposes of describing size-change termination we'll consider a language
\mono{D}. The following chapter describes the syntax and semantics of the
language.

\section{General properties}

The intent of the language is for it be used to explain concepts such as
size-change termination. One of the fundamental concepts required of the
language of application is that it's datatypes are well-founded. That is, any
subset $S$ of the range of values of some well-defined type has a value $s$
s.t. $\forall {s'\in S}\ s\leq s'$. This makes it ideal to chose some
oversimplistic data type structure rather than an army of basic types. Besides,
an apropriately defined basic data type should be able to represent arbitrarily
complex data values.

The language is initially first-order since the size-change termination
principle is first described for first-order programs later on in this work.
However, the language is designed so that it is easy to turn it into a
high-level language without much effort. This may prove necessary as we try to
expand size-change termination to higher-order programs.

The language is a call-by-value and purely functional to avoid any problems
that could arise from regarding lazy programs or where the notion of a global
state of the machine is relevant. Simply put, this is done to ensure elegance
of further proof with the help of the language.

\input{language/syntax}
\input{language/semantics}
\input{language/old}

\section{Data}

\subsection{Representation}\label{section:d-data-representation}

The language \mono{D} is untyped, and represents all data in terms of
\emph{unlabeled ordered binary trees}, henceforth referred to as simply,
\emph{binary trees}. Such a tree is recursively defined as follows:

\begin{definition}

A binary tree is a set that is either empty or contains a single labelless node
with a left and right child.

\end{definition}

For simplicity, we'll sometimes refer to the empty set as a \emph{leaf}, and
the singleton set as a \emph{node}.

To operate on such trees we'll require a few underlying capabilities. Below
we'll define these capabilities in general for the purpose of analysis. In
\referToSection{d-syntax} we'll define the concrete syntax for these in
\mono{D}.

First and foremost, we need to represent leaves, we do this using the atom
$leaf$. Furthermore, we'll define the following operations:

\begin{equation}
{
{}\over{
\proc{Construct}(A_{left}, A_{right})
\longrightarrow
A
}}
\ \ \ \ \ \ \text{If $A_{left}$ and $A_{right}$ are the respective children
of $A$.}
\end{equation}

\begin{equation}
{
{}\over{
\proc{Destruct}(A)
\longrightarrow
\{A_{left},A_{right}\}
}}
\ \ \ \ \ \ \text{If $A_{left}$ and $A_{right}$ are the respective children
of $A$.}
\end{equation}

\begin{equation}
{
{}\over{
\proc{IsNode}(A)
\longrightarrow
true
}}
\ \ \ \ \ \ \text{If $A$ is a node.}
\end{equation}

\begin{equation}
{
{}\over{
\proc{IsNode}(A)
\longrightarrow
false
}}
\ \ \ \ \ \ \text{If $A$ is a leaf.}
\end{equation}

\begin{equation}
{
{
\proc{IsNode}(A)
\longrightarrow
true
}\over{
\proc{IsLeaf}(A)
\longrightarrow
false
}}
\end{equation}

\begin{equation}
{
{
\proc{IsNode}(A)
\longrightarrow
false
}\over{
\proc{IsLeaf}(A)
\longrightarrow
true
}}
\end{equation}

Although we won't be needing $\proc{IsNode}$ and $\proc{IsLeaf}$ once the
concrete syntax of \mono{D} is in place (since it makes use of pattern
matching), it is still worth defining what we'll regard as the values $true$
and $false$, for the sake of other functions. We'll adopt a C-like convention:

\begin{definition}\label{definition:true-false}

Any non-$leaf$ binary tree represents the value $true$ and any $leaf$
represents the value $false$.

\end{definition}

\subsection{Size}

For the purposes of talking about size-change termination, we also need to
define the notion of size, and be sure to do so in such a way so that all
possible data values are well-founded.

\begin{definition}\label{definition:size}

Size of a value in \mono{D} is the number of nodes in the tree representing
that value.

\end{definition}

The ``well-foundedness'' of \mono{D}'s data values, given such a definition can
be argued for by equating the definition to a mapping of \mono{D}'s data values
onto the natural numbers. This would imply that we can define the relation $<$
on \mono{D}'s data values, which we know to be well-founded. 

This is a bijection

We start by formally proving that \referToDefinition{size} yields a many-to-one
mapping of \mono{D}'s data values to the natural numbers.

First, we prove, by induction, that any natural number can be represented in
\mono{D}:

\begin{proof}\ \\

\begin{description}[\setleftmargin{70pt}\setlabelstyle{\bf}]

\item [Base] A $leaf$ has no nodes, and hence represents the value $0$.

\item [Assumption] If we can represent the $n\in\mathbb{N}$ in \mono{D}, then
we can also represent the number $n+1\in$. 

\item [Induction] Let $n$ be represented by some binary tree $A$, then $n+1$
can be represented by $\proc{Construct}(leaf,A)$. 

\end{description}

\end{proof}

Second, we prove, also by induction, that any value \mono{D} has a
representation in $\mathbb{N}$.

\begin{proof}\ \\

\begin{description}[\setleftmargin{70pt}\setlabelstyle{\bf}]

\item [Base] A $leaf$ has no nodes, and hence corresponds to the value $0$.

\item [Assumption]

\begin{enumerate}

\item If the binary tree $A$ has a representation $n\in\mathbb{N}$, then 
$\left|\proc{Construct}(leaf, A)\right|\equiv n+1$ and
$\left|\proc{Construct}(A, leaf)\right|\equiv n+1$.

\item If the binary tree $A$ has a representation $n\in\mathbb{N}$, and the
binary tree $B$ has a representation $m\in\mathbb{N}$, then
$\left|\proc{Construct}(A,B)\right|\equiv n+m+1$ and
$\left|\proc{Construct}(B,A)\right|\equiv n+m+1$.

\end{enumerate}

\item [Induction]

By definition of the binary operation $\proc{Construct}$,

$$\left|A\right|=1+\left|A_{left}\right|+\left|A_{right}\right|$$

Hence, the assumptions must hold.

\end{description}

\end{proof}

\referToDefinition{size} \emph{almost} allows us to devise an algorithm to
compare the sizes of data values. The problem withstanding is that two
different values can have rather diverging tree representations. Hence,
comparing them, using only the operations defined in
\referToSection{d-data-representation}, is seemingly impossible unless we
initially, or along the way, transform the binary trees being compared into
some sort of a \emph{standard representation}. We'll define this
representation, recursively, as follows:

\begin{definition}\label{definition:standard-representation}

A binary tree in standard representation is a binary tree that either is a leaf
or a node having a leaf as it's left child and a binary tree in standard
representation as it's right child.

\end{definition}

Intuitively, a binary tree in standard representation is just a tree that only
descends along the right side. Comparing the sizes of two trees in this
representation is just a matter of walking the descending in the two trees
simultaneously, until one of them, or both, bottom out. If there is a tree that
bottoms out strictly before another, that is the lesser tree by
\referToDefinition{size}.

\subsection{$\proc{Less}(A,B)$}

Assuming that we've already defined a procedure $\proc{Normalize}$ for
transforming an arbitrary tree into it's standard representation (which we'll
do further below), we can define the procedure $\proc{Less}(A,B)$ for
determining whether the value of the binary tree $A$ is strictly less than the
value of binary tree $B$, as follows:

\begin{codebox}
\Procname{$\proc{Less}(A,B)$}
\li $\proc{NormalizedLess}(\proc{Normalize}(A),\proc{Normalize}(B))$
\end{codebox}

\begin{codebox}
\Procname{$\proc{NormalizedLess}(A,B)$}
\li \If $\proc{IsLeaf}(A)$ \Then\label{normalized-less-init-start}
\li   \Return $\proc{IsNode}(A)$
    \End
\li \If $\proc{IsLeaf}(B)$ \Then
\li   \Return $false$
    \End\label{normalized-less-init-end}
\zi
\li $\{\_, A_{right}\} \gets \proc{Destruct}(A)$
\li $\{\_, B_{right}\} \gets \proc{Destruct}(B)$
\zi
\li \Return $\proc{NormalizedLess}(A_{right},B_{right})$
\end{codebox}

\subsubsection{Correctness}

\begin{proof}

Given \referToDefinition{standard-representation}, and the assumption that
$\proc{Normalize}(A)$ computes the standard representation of $A$, we know the
following:

\begin{enumerate}

\item $\left|A\right|\equiv\left|\proc{Normalize}(A)\right|$.

\item We'll walk through all the nodes if we perform a recursive
right-child-walk starting at $A$.

\item The same holds for $B$.

\end{enumerate}

It is also easy to see from lines
\ref{normalized-less-init-start}:\ref{normalized-less-init-end} that
$\proc{NormalizedLess}$ stops as soon as we reach the ``bottom'' of either $A$
or $B$.

Given \referToDefinition{size}, $A<B$ iff it bottoms out before $B$, that is,
we reach an instance of the recursion where both $IsLeaf(A)$ and $IsNode(B)$
hold.  In all other cases $A\geq B$, the cases specifically are:

\begin{itemize}

\item $IsLeaf(A)$ and $IsLeaf(B)$, then $\left|A\right|\equiv \left|B\right|$.

\item $IsNode(A)$ and $IsLeaf(B)$, then $\left|A\right|>\left|B\right|$

\end{itemize}

Last but not least, due to all data values being finite, eventually one of the
trees does bottom out.

\end{proof}

\subsubsection{Time complexity}

Given that the binary trees $A$ and $B$ are in standard representation when we
enter the auxiliary procedure, $\proc{NormalizedLess}$, it is fairly easy to
get an upper bound on the running time of $\proc{NormalizedLess}$ itself.

Indeed, the running time of $\proc{NormalizedLess}$ itself is
$O\left(\proc{Max}\left(\left|A\right|,\left|B\right|\right)\right)$, since we
just walk down the trees until one of them bottoms out. 

We haven't yet defined the procedure $\proc{Normalize}$ yet. Hence, the only
thing that we can say about the running time of $\proc{Less}$ in general is
that it is $O\left(\proc{Normalize}(A) + \proc{Normalize}(B) +
\proc{Max}\left(\left|A\right|,\left|B\right|\right) \right)$.

\subsubsection{Space complexity}

Coming soon..

\subsection{$\proc{Normalize}(A)$}

To complete the definition and analysis of $\proc{Less}$ we need to define and
analyze $\proc{Normalize}$. We do this below.

\begin{codebox}
\Procname{$\proc{Normalize}(A)$}
\li $\proc{NormalizeAuxiliary}(A,leaf,leaf)$
\end{codebox}

\begin{codebox}
\Procname{$\proc{NormalizeAuxiliary}(A,A',A_{normalized})$}
\li \If $\proc{IsNode}(A)$ \Then
\li   $A_{normalized}\gets \proc{Construct}(leaf,A_{normalized})$
\li \Else
\li   \Return $leaf$
    \End
\zi
\li $\{A_{left}, A_{right}\} \gets \proc{Destruct}(A)$
\li \If $\proc{IsNode}(A_{left})$ \Then
\li   $A'\gets \proc{Construct}(A_{left}, A')$
    \End
\li \If $\proc{IsNode}(A_{right})$ \Then
\li   $A \gets A_{right}$
\li \Else
\li   \If $\proc{IsNode}(A')$ \Then
\li     $\{A,A'\} \gets \proc{Destruct}(A')$
\li   \Else
\li     \Return $A_{normalized}$
      \End
    \End
\li \Return $\proc{Normalize}(A,A',A_{normalized})$
\end{codebox}

\subsubsection{Correctness}

Coming soon..

\subsubsection{Time complexity}

Coming soon..

\subsubsection{Space complexity}

Coming soon..

\subsection{Size}

Althought the language is already complete, it would prove useful for further
analysis to define the notion of \emph{size}, and hence the equality and order
of relations on data values. Without further a-do, we define the size of a
value to be \emph{the number of nodes in the binary tree}.

The euqality and order relations are a bit more complicated though, as that
there is seemingly no \emph{elegant} way to decide whether one arbitrary binary
tree has the same or ..

Hence, the tree \mono{0} has the value $0$, the tree \mono{0.0} has the value
$1$, and the tree \mono{0.0.0} has the value $2$ as does it's symmetrical
equivalent, \mono{(0.0).0}.

This allows us to define the, otherwise built-in, function \mono{less} in a
primitive recursive fashion as follows:

\begin{verbatim}
less 0 0 = 0
less _._ 0 = 0
less 0 _._ = 0.0
less A.B X.Y = 

less AR.AL BR.BL = or (and (less AR BR) (less AL BL))
\end{verbatim}

This definition indicates that we choose for the empty tree to represent the
value \emph{false}, and for the tree \mono{0.0} to represent the value
\emph{true}. We'll keep the definition even more generic, and let the
\emph{nonempty} tree represent the value \emph{true}, as shall become useful
when we define the higher-order function \mono{if}
(\referToSection{language-higher-order-built-ins}).

Since the values begin at $0$ and grow at the rate of $1$ ... we can define it
as syntactic sugar and use nonnegative integers where ...

In addition to defining the actual data type we need to specify how we're going
to reason about it. Specifically, the questions of equality and order of values
constructed in this manner have to be answered.

For all intents and purposes, we can let the \emph{absolute value} of such a
tree-structured value be equal to $n-1$, where $n$ is the number of leafs in
the tree. Hence, the tree \mono{0} denotes $0$, \mono{0.0} denotes 1,
\mono{0.0.0} denotes 2 and so on.

The choice of this data representation yields the following properties for the
construction and destruction operators:

\begin{lemma} Construction of value yields a value strictly greater than either
of it's constituents. Specifically, the absolute value of the new value is the
sum of the absolute values of the constituents.\end{lemma}

\begin{lemma} Destruction of a value yields a pair of values who's absolute
values are strictly less than the absolute value of the original
value.\end{lemma}

\subsection{Programs}

Programs are defined in a conventional functional context and without mutual
recursion, namely:

\begin{align}
\nonterm{program}\ ::=&\ \nonterm{function}^*\ \nonterm{expression}
\end{align}

The order of the function definitions does matter wrt. pattern matching in so
far as those defined before are attempted first, if the match fails, the next
function with the same signature\footnote{In this case comprising of the name
of the function and it's arity.} is attempted.

Note, that we let the number of function definitions be zero as an
$\nonterm{expression}$ is a valid program as well. More generally, the program
can be thought of as a constant function, where the actual
$\nonterm{expression}$ simply has access to some predefined functions defined
by the function definitions in the program.

\subsection{Built-in high-order
functions}\label{section:language-higher-order-built-ins}

Although \mono{D} is initially a first-order language, we will ignore that
limitation for a bit and define a few higher-order functions to provide some
syntactical sugar to the language. Beyond the discussion in this section, these
higher-order functions should be regarded as \mono{D} built-ins.

\subsubsection{Branching}

In the following definition, the variable names \mono{true} and \mono{false}
refer to expressions to be executed in either case.

\begin{verbatim}
if 0 _ false := false
if _._ _ true := true
\end{verbatim}

As you can see, we employ the C convention that any value other than $0$ is a
``truthy'' value, and the expression \mono{true} is returned.

Although the call-by-value nature of the language does not allow for
short-circuiting the if-statements defined in such a way, this shouldn't be any
impediment to further analysis.

\subsection{Sample programs}

As an illustration of the language syntax, the following program reverses a tree:

\begin{verbatim}
reverse 0 := 0
reverse left.right := (reverse right).(reverse left)
\end{verbatim}

The following program computes the Fibonacci number \mono{n}:

\begin{verbatim}
fibonacci 0 x y := 0
fibonacci 0.0 x y := y
fibonacci n x y := fibonacci (minus n 0.0) y (add x y)
\end{verbatim}

\section{Semantics}

In the following section, the operational semantics of the language \mono{D}
are defined in terms of structured operaitonal semantics\cite{sos}.
\referToTable{sos-definitions} specifies most\footnote{The rest is discussed
further below and in \referToSection{language-semantics-memory}.} of the
syntactical elements used to define the semantic reduction rules below.

In addition to the notation specified in the table, we'll make use of
Haskell-like list comprehension when dealing with lists of elements. For
instance, $[e]$ refers to a list of expressions, and $[e'|e]$ refers to a list
of expressions that starts with the expression $e'$ and is followed by the list
of expressions, $e$. Also a bit alike Haskell, in both cases above, $e$ is used
as both a type and a variable.

\makeTable
{sos-definitions}
{Some of the syntactical elements used in the reduction rules for \mono{D}.}
{|lll|}
{\textbf{Notation}&\textbf{Description}}
{
$e$ & expression\\
$v$ & value\\
$n$ & variable name\\
$p$ & pattern\\
$0$ & the atom $0$\\
$\cdot$ & $\term{.}$\\
$\sigma$ & memory\\
$\sigma[n]$ & the value of variable $n$ in memory, returns some $v$
}

\subsection{Memory}\label{section:language-semantics-memory}

For the sake of an elegant notation, we'll define the notion of memory as a set
of stacks, one for each variable in the program. If a variable $n$ has an empty
stack, it is undefined, otherwise the value of the variable (in the present
scope) is the value at the top of the corresponding stack.

This model of memory allows us to deal with abitrary scope\footnote{Although
first-order \mono{D} can make little use of that.} in a rather elegant matter,
where we simply push a new value onto the corresponding stack when we enter a
nested scope and pop off the corresponding stacks when exiting a nested scope.
Hence, visiting a nested scope has the following operational semantics wrt.
$\sigma$:

$$\sigma\longrightarrow\sigma(n)\leftarrow v\longrightarrow\sigma$$

Following the conventions of structured operational semantics, the value of an
element, such as $\sigma$, does not change throughout a reduction rule. So in
the above example, the starting $\sigma$ is equivalent to the final $\sigma$.

\subsection{Evaluation}

\mono{D} has only one operator, namely $\term{.}$, which is a right-associative
binary operator, hence expressions are evaluated using the following triplet of
rules:

\begin{equation}
{
\left\langle e',\ \sigma\right\rangle
\longrightarrow
\left\langle e'',\ \sigma\right\rangle
}\over {
\left\langle e \cdot e',\ \sigma\right\rangle
\longrightarrow
\left\langle e \cdot e'',\ \sigma\right\rangle
}
\end{equation}

\begin{equation}
{
\left\langle e,\ \sigma\right\rangle
\longrightarrow
\left\langle e',\ \sigma\right\rangle
}\over {
\left\langle e\cdot v,\ \sigma\right\rangle
\longrightarrow
\left\langle e' \cdot v,\ \sigma\right\rangle
}
\end{equation}

\begin{equation}
\left\langle v \cdot v',\ \sigma\right\rangle
\longrightarrow
\left\langle v'',\ \sigma\right\rangle
\ \ \ \ \ \ (\text{where}\ v'' = v \cdot v')
\end{equation}

\subsubsection{Variables}

As expressions may contain variable names, we need a way to retrieve the values
of variables from memory:

\begin{equation}
\left\langle n,\ \sigma\right\rangle
\longrightarrow
\left\langle \sigma[n],\ \sigma\right\rangle
\end{equation}

\subsubsection{Application}

Function application is left-associative and has variable (constant at run
time) arity of at least one. The arity of the application depends on the
declaration that the function specifier, $f$, points to. Hence, we begin by
evaluating the function specifier itself to some expression $\lambda$ and a
pattern list $[p]$, that is, it's corresponding function declaration:

\begin{equation}
{
\left\langle f,\ \sigma\right\rangle
\longrightarrow
\left\langle \left\langle \lambda,\ [p]\right\rangle,\ \sigma\right\rangle
}\over{
\left\langle \left\langle f,\ [e]\right\rangle,\ \sigma\right\rangle
\longrightarrow
\left\langle
\left\langle\left\langle \lambda,\ [p]\right\rangle,\ [e]\right\rangle
,\ \sigma\right\rangle
}
\end{equation}

\emph{Note, in a first-order context $\lambda$ bares no special meaning,
however, the letter is carefully chosen to aid further extension of \mono{D} to
it's higher-order sibling.}

Followed by evaluation of the pattern and expression lists:

\begin{equation}
{
\left\langle p',\ \sigma\right\rangle
\longrightarrow
\left\langle p'',\ \sigma\right\rangle
\wedge
\left\langle e',\ \sigma\right\rangle
\longrightarrow
\left\langle e'',\ \sigma\right\rangle
}\over{
\left\langle p\cdot p',\ e\cdot e',\ \sigma\right\rangle
\longrightarrow
\left\langle p\cdot p'',\ e\cdot e'',\ \sigma\right\rangle
}
\end{equation}

\begin{equation}
{
\left\langle e',\ \sigma\right\rangle
\longrightarrow^*
\left\langle 0,\ \sigma\right\rangle
}\over{
\left\langle p\cdot 0,\ e\cdot e',\ \sigma\right\rangle
\longrightarrow
\left\langle p,\ e,\ \sigma\right\rangle
}
\end{equation}

\begin{equation}
{
\left\langle e',\ \sigma\right\rangle
\longrightarrow^*
\left\langle v',\ \sigma\right\rangle
}\over{
\left\langle p\cdot n,\ e\cdot e',\ \sigma\right\rangle
\longrightarrow
\left\langle p,\ e,\ \sigma(n)\leftarrow v' \right\rangle
}
\end{equation}

%\begin{equation}
%{
%\left\langle p',\ \sigma\right\rangle
%\longrightarrow
%\left\langle p'',\ \sigma\right\rangle
%\wedge
%\left\langle e',\ \sigma\right\rangle
%\longrightarrow
%\left\langle e'',\ \sigma\right\rangle
%}\over{
%\left\langle p'\cdot n,\ e'\cdot n,\ \sigma\right\rangle
%\longrightarrow
%\left\langle p''\cdot n,\ e''\cdot n,\ \sigma(n)\leftarrow v' \right\rangle
%}
%\end{equation}



We complete application by evaluating $\lambda$ with the new memory state:

\begin{equation}
\left\langle \lambda,\ \sigma([n])\leftarrow[v]\right\rangle
\longrightarrow
\left\langle v',\ \sigma\right\rangle
\end{equation}

\emph{Note, we return to the original $\sigma$ once $\lambda$ is evaluated.}

\newpage

This is small-step..

\begin{equation}
{
\left\langle n_f, [e], \sigma\right\rangle
\longrightarrow
\left\langle e_f, [p], [e], \sigma\right\rangle
\ \ \ \ \ \text{where}\ \left\langle e_f, [p] \right\rangle
\ \text{is the definition of the function with the name}\ n_f\text{.}
\over
\left\langle n_f, [e], \sigma\right\rangle
\longrightarrow
\left\langle v, \sigma\right\rangle
}
\end{equation}
