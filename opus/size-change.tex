\chapter{Size-change termination}

The size-change termination analysis builds upon the idea of flow analysis of
programs. In general, flow analysis aims to answer the question, ``What can we
say about a given point in a program without regard to the execution path taken
to that point?''. A ``point'' in a computer program is in this case a primitive
operation such as an assignment, a condition branch, etc.

The idea is then to construct a graph where such points are nodes, and the arcs
in between them represent a transfer of control between the primitive
operations, that would otherwise occur under the execution of the program.
Such a node may have variable in-degree and out-degree from one given
primitive. For instance, a condition branch would usually have two possible
transfers of control depending on the outcome of the condition. Hence, it
serves useful to label arcs depending on when they are taken. The conditions
should clearly not overlap to avoid non-determination.

Such graphs are referred to as \emph{control flow graphs}.

With such a graph at hand, various optimization algorithms can be devised to
traverse the graph and deduce certain properties, such as reoccurring primitive
operations on otherwise static variables\cite{kildall}, etc.

\section{Control flow graphs in \D{}}

\subsection{Start and end nodes}

Every control flow graph has a start and an end node. These nodes do not
explicitly represent control primitives, but rather the start and end of a
program. \emph{A program cannot be started nor ended more than once}. The start
node is labelled $S$ and has out-degree $1$ and in-degree $0$. The end node is
labelled $E$ and has out-degree $0$, but variable in-degree, i.e. a program can
be ended in more than one way.

The control-flow graph for the empty program is hence:

\input{figures/cfg-empty}

\subsection{Function clauses}

While node construction and destruction are primitive operations in \D{}, we'll
refrain ourselves from delving into such details in the control flow graphs of
our programs. Indeed because \emph{node construction and destruction always
terminates}. Instead, we'll let a \emph{function clause} define a point in a
program.

The expression of the clause can thereafter make calls to its
enclosing\footnote{We say that a function \emph{consists} of function clauses
and a function clause is \emph{enclosed} in a function.}, or some other
function. Such calls are represented by transfer of control, that is, arcs.
Disregarding the cases where a function clause expression makes multiple calls
to the same function with different arguments, these arcs need not be
disjunctively labelled since all of these transitions happen unconditionally as
a result of evaluating the expression. More specifically, \emph{we consider the
order of evaluation to be insignificant}, and hence undeserving of labelling.
We further discuss the reasons for this below.

If calls are separated by node construction, the order in which those calls are
made is definitely insignificant. For instance, consider the expression
\mono{(f a).(g b)}, where \mono{f} and \mono{g} are some well-defined
functions, $\text{\mono{f}}\neq\text{\mono{g}}$, and \mono{a} and \mono{b} are
some bound variables. It makes no difference to the final result which of the
calls, \mono{f a} and \mono{g b}, is evaluated first. Indeed, they can be
evaluated in parallel, and we would still get the same result. This is easy to
see for any nested construction of results of function calls, as in e.g.
\mono{(f a).0.(g b)}.

On the other hand, the syntax and semantics of \D{} allow for function calls to
be nested as in e.g. the expression \mono{(f (g a) (h b))}, where \mono{h} is
also some well-defined function and is pairwise unequal to \mono{f} and
\mono{g}. While the order of evaluation of \mono{g a} and \mono{h b} is
\emph{insignificant} wrt. to one another, as with function calls separated by
construction, the order of evaluation of these two subexpressions wrt. to the
call to function \mono{f}, \emph{is significant to the result}, and
\emph{might} be significant to termination analysis in general. However, we'll
regard this as insignificant for the time being for mere simplicity. We'll come
back to the question of whether size-change termination analysis can benefit
from regarding this as significant later on.

We can now draw a control flow graph for the program define in
\referToListing{cfg-sample-1} as shown in \referToFigure{cfg-sample-1}.

\begin{lstlisting}[label=listing:cfg-sample-1,caption={A sample \D{} program, always returning \mono{0.0.0}.}]
f x y := x.y
g _ := 0
h _ := 0
i x y := (f ((h y).(g x)) (h y))
i input input
\end{lstlisting}

\input{figures/cfg-sample-1}

\subsection{Call cycles}

A call cycle occurs when there is a cyclical transition of control between the
nodes of a control flow graph. I.e. when there is a cycle in the control flow
graph.

\begin{lemma} We're concerned with call cycles in control flow graphs since
non-termination cannot occur if not for an infinite control flow cycle.
\end{lemma}

\begin{proof} If a program has a control flow graph with no cycles and does not
terminate, then one of the primitive operations, i.e. construction,
destruction, comparison or binding, does not terminate, which is certainly
absurd given the semantics of \D{}. \end{proof}

Call cycles in \D{} can occur in recursive or mutually recursive function
clauses.

We will henceforth refer to function clauses with recursive calls as
\emph{recursive clauses} and their counterparts, i.e. the base clauses of a
function declaration, \emph{terminal clauses}.

\subsection{Disregarding back-propagation}

It is worth noting that in \referToFigure{cfg-sample-1}, the clauses that make
no function calls have out-degree $0$. Technically, these functions \emph{do
transfer control} -- back to the callee. We may refer to this process as
\emph{back-propagation of control}. While considering back-propagation is
seemingly important to a concept that bases itself on the changes in the sizes
of the program values, we're only concerned with call cycles.

The thing with back-propagation is that forward-propagation after
back-propagation of a call cannot occur due to the way \D{} is defined. Hence,
what we are really concerned with is, ``how deep the rabbit hole goes'', before
we back-propagate, as back-propagation superimplies termination of the function
we're back-propagating out of.

\subsection{Dropping the start and end nodes}

The disregard of the back-propagation of control forces us to either redefine
the transition from the start node and the transitions to the end node. This is
because neither of these transitions are ever back-propagated, while all other
transitions \emph{must be} back-propagated if the program terminates.

Alternatively, disregard of back-propagation allows us to drop these nodes
completely and concentrate on the clauses and explicit calls within the clause
expressions. Hence, start and end nodes will not appear in any further graphs.

\subsection{Control flow graphs vs. abstract static call graphs}

Disregard of back-propagation allows us to consider control flow graphs
presented in this text as mere \emph{abstract static call graphs}, henceforth
referred to simply as, \emph{call graphs}. The abstraction applied to these
graphs compared to regular static call graphs is that the concrete arguments of
the function calls are not considered, and we merely consider how these values
can change in size from for a given function call. Interestingly, the problem
of termination analysis can be rephrased as the problem of determining whether
the regular static call graph of a program, i.e. the one containing the
concrete function arguments, is finite.

\subsection{Multiple calls to the same function}

Up until now we've only regarded expressions that don't make calls to the same
function with varying arguments. This is because these calls have to be
disjunctively labelled for the purposes of our analysis, because the use of
varying arguments \emph{may} mean varying decrease (or increase), in values for
the different calls within the expression. For this purpose we'll disjunctively
label \emph{all} the calls within an expression, if necessary, but remember
that this has nothing to do with evaluation order as has been discussed above.

This allows us to draw a control flow graph, or equivalently, a call graph,
for the program in \referToListing{cfg-sample-2}. Here, we've already
disjunctively labelled all of the calls in the expressions. This call graph is
drawn in \referToFigure{cfg-sample-2}.

\begin{lstlisting}[label=listing:cfg-sample-2,caption={A sample \D{} program, always returning \mono{(0.x).(0.y)}, where \mono{x} and \mono{y} are arbitrary \D{} values supplied by the user.}]
f x y := x.y
g x := 0.x
i x y := (0: f (1: g x) (2: g y))
i input input
\end{lstlisting}

\input{figures/cfg-sample-2}

\subsection{Multiple clauses}

If function clauses are nodes, and the function calls within the expressions of
the function clauses are unconditional transitions, what exactly happens if the
arguments supplied to the function clause fail to match the pattern declaration
for the clause?

The semantics of \D{} tell us to make an unconditional transition to the
immediately next clause of the function. There is at most one such transition
for any clause, and the last clause of a function declaration cannot fail to
pattern match\footnote{See \referToSection{d-sos}.}.

We'll refer to these transitions as \emph{fail transitions} and visually mark
them with a dotted line rather than a filled line. We need this way of visually
distinguishing fail transitions from the rest since they are conditionally
different, in that for any clause with a fail transition, either the fail
transition is chosen, or all the non-fail transitions are chosen
simultaneously.

Before we can draw the call graph we also need a way to distinguish the clauses
of a function wrt. the program text. We decide to enumerate the clauses
top-to-bottom starting with 0. Sometimes we'll annotate the program text with
these unique labels for each clause to make the call graph more readable.

Hence, we can now draw the call graph for the program defined in
\referToListing{cfg-loop} as shown in \referToFigure{cfg-loop}.

\begin{lstlisting}[label=listing:cfg-loop,caption={A simple, down-counting loop in \D{}.}]
f0: f 0 := 0
f1: f x._ := f x

f input
\end{lstlisting}

\input{figures/cfg-loop} 

For a more complex example, let's consider the call graph for the program
\mono{reverse} introduced in \referToSection{d-samples}. The program is
repeated in annotated form in \referToListing{cfg-reverse}, and its
corresponding call graph is shown in \referToFigure{cfg-reverse}.

\begin{lstlisting}[label=listing:cfg-reverse,caption={An annotated version of the program \mono{reverse} introduced in \referToSection{d-samples}.}]
r0: reverse 0 := 0
r1: reverse left.right := (0: reverse right).(1: reverse left)

reverse input
\end{lstlisting}

\input{figures/cfg-reverse} 

\section{Size-change termination principle}

Consider the program in \referToListing{cfg-loop} and it's corresponding call
graph in \referToFigure{cfg-loop}. Without any further information about the
control transitions, the program seemingly loops indefinitely. However,
some things we do know about the control transitions, and others we can
deduce.

What we would like to know for any control transition is \emph{how the
variables bound in the source relate to the variables bound in the
destination}. The source and destination in this case are function clauses. We
will denote this relation with the letter $\nabla$, s.t. if the clause $f_0$ is
the source and the clause $g_0$ is the destination, we would like to construct
the relation $f_0\nabla g_0$.

According to the semantics of \D{}, when a function call occurs, the source
evaluates the arguments of the function call and generates some \emph{values}.
The values may hence be a nested construction of other concrete values, values
of variables, and results of nested function calls. This implies that a
relation can be deduced between the variables bound in the source and the
values that result from the evaluation. That relation need not be precise in
the sense of size displacement, but it suffices for initial size-change
termination analysis to know if the value of the variable is strictly increased
or perhaps equivalent to the final value.

This is followed by the source transmitting the resulting values to the
destination, i.e.  performing a control transition and the destination pattern
matching the incoming values, the result of which may be binding of variables
in the destination clause.

Variables (or names) are bound to values, hence the relation $\nabla$ is a
injective relation between variable names, where each mapping is annotated with
the change in size indicating a lower bound on the difference between the
values of the variables.

In the following sections we discuss how we can deduce the lower bound on the
difference between the values of the variables.

\subsection{Fail transitions}

If pattern matching fails for a given clause, a fail transition occurs, where
the values originally sent to given clause are simply forwarded to the next
clause.

\begin{lemma} Fail transitions are transitive in the sense that the
relationship between the variables bound in the source and the variables bound
in the destination is the same regardless of the number of fail transitions in
the path between the source and the destination.\end{lemma}

\begin{proof} Follows from the semantics of \D{}. \end{proof}

Note, that due to \D{} being first order and statically scoped, the variable
space is always initially empty when a function clause begins pattern matching.

\subsection{Pattern matching}

\begin{definition} For the sake of further discussion we'll regard the
underscore pattern simply as a uniquely named variable that is not used in the
expression of the function clause, i.e. it is also bound to a
value.\end{definition}

\begin{lemma}\label{lemma:d-pattern-leq} If the single pattern declaration
includes a variable name, we can safely state that if some variable is bound as
a result of pattern matching the corresponding argument, the value that
variable is bound to is less than or equal to the value of the original
argument. \end{lemma}

\begin{proof} There is no construction operator for a pattern
declaration.\end{proof}

\begin{lemma}\label{lemma:d-pattern-less} If a destruction operator
participates in a pattern declaration, then any variable bound as a result of
pattern matching the corresponding argument will be bound to a value strictly
less than the value of the original argument.\end{lemma}

\begin{proof} A variable can be bound to the actual value of the argument iff
the entire pattern is just a variable name. If the pattern contains at least
one destruction operator, the syntax rules of \D{} cause for that operation to
be performed first when pattern matching the argument. Hence, any value that
can be bound thereafter has a value with at least one node less than the
original value of the argument. \end{proof}

Hence, we can deduce from \referToListing{cfg-loop}, that when $f_1$ makes a
call to $f_0$ it does so with a value strictly less then it's own argument,
i.e. the transition $f_1\rightarrow f_0$ strictly decreases a value. Visually
we will mark this with a $\downarrow$. The Lemmas \refer{lemma:d-pattern-leq}
and \refer{lemma:d-pattern-less} can be used to deduce the same sort of
relationship for the transitions $r_1\xrightarrow{0,1} r_0$ for
\referToListing{cfg-reverse}. These observations are summarised in
\referToFigure{sct-first}.

\input{figures/sct-first}

\subsection{Calls to multivariate functions}

The call graph notation used thus far has only been used for describing calls
to unary functions. As an example of a multivariate function, we may consider
the function \mono{normalized-less/2}, introduced in
\referToSection{d-size-less}. We use this function to define the program in
\referToListing{sct-normalized-less}. The corresponding call graph is shown in
\referToFigure{sct-normalized-less}.

\begin{lstlisting}[
  label={listing:sct-normalized-less},
  caption={A sample program with a multivariate function.}]
n0: normalized-less 0 b := b
n1: normalized-less _ 0 := 0
n2: normalized-less _.a _.b := normalized-less a b
normalized-less input input
\end{lstlisting}

\input{figures/sct-normalized-less}

The notation is straightforward, the juxtaposition of the $\downarrow$
indicates the size change of the respective arguments, read left to right as in
the function clause definition.

\subsection{Nonincreasing transitions}

There are cases where for a given transition in a call cycle, we can't tell
whether the sizes are strictly decreased or remain the same, but we can
definitely say that there is \emph{no increase} in size.

As an example, consider the program in \referToListing{sct-non-reducing}.

\begin{lstlisting}[
  label=listing:sct-non-reducing,
  caption={A binary function where a strict decrease in value is uncertain
    for some transition in a call cycle.}]
g0: g 0 0 = 0
g1: g _.a b._ = g 0.a b.0
g input input
\end{lstlisting}


For the recursive clause $g_1$, it is unclear whether
the sizes of the variables are decreased in the transition $g_1\rightarrow g_0$, or not.
Specifically, if the arguments to \mono{g} are of the form \mono{0.\_} and
\mono{\_.0}, respectively, the size is \emph{not} decreased by the call. We'll
denote such transitions by the symbol $\Downarrow$. We can now draw the call
graph for the program in \referToListing{sct-non-reducing}. The call graph is
shown in \referToFigure{sct-non-reducing}.

\input{figures/sct-non-reducing}

\subsection{Increasing transitions}


%Values are not always decreasing.

%Consider the following program:

\begin{lstlisting}[
  label=listing:sct-infinite-join,
  caption={The function \mono{infinite-join/2} infinitely joins..}]
f0: f a b = f a.b b.a
f input input
\end{lstlisting}

%The function clause expression constructs a new value by joining the two
%incoming arguments into a new node. While we're unable to say by how much the
%values are increased exactly, we know that both values are increased by at
%least 1 due to the creation of a node for each argument.


%\newpage

%A derrivable property from the program text is that the value sent as the
%argument in the transition $F_1\rightarrow F_0$ is \emph{always} strictly less
%than the value sent as the argument in the preceeding transition,
%$F_0\rightarrow F_1$. Additionally, we can universally state that \emph{false
%transitions from case nodes by definition can't change any values}. Hence, the
%infinite control flow sequence $F_0\rightarrow F_1\rightarrow F_0$ strictly
%decreases a well founded value in every iteration of the control flow cycle.
%This means that eventually the value bottoms out at $0$ which would validate
%the $F_0$ pattern and lead the program to terminate. In general, we can state
%the following:

%\begin{lemma}

%An infinite control flow sequnece infinitely decreases a well founded value if
%all the transitions in the corresponding control flow graph cycle either
%strictly decrease a value or keep all values unchanged, and at least one
%transition strictcly decreases a value.

%\end{lemma}

%\referToFigure{cfg-loop-down} showcases an edge-labelled control flow graph for
%\referToListing{d-loop}. Transitions for which we can \emph{safely} say that a
%value is decreased we mark with the symbol $\downarrow$, transitions for which
%we can \emph{safely} say that no value is increased we mark with the symbol
%$\top$. The graph is rather verbose and in future graphs we'll omit $\top$
%labels for nodes that universally can't increase a value, i.e. $S\rightarrow
%F_0$, $F_0\rightarrow E$ and $F_0\rightarrow F_1$.

%We can further prove the soundness of the size-change termination principle.

%\begin{proof} Assume for the sake of contradiction that there exists an
%infinite control flow sequence that infinitely decreases a well-founded data
%value. Assume furthermore that the program does not terminate. Then, some
%variable has to decrease indefinately, which contradicts with the definition of
%well-founded values. \end{proof}

%Unfortunately, this particular program can be trivially unfolded into a loop
%program, so the point of using size-change termination analysis may seem
%superflous. Indeed, the only change wrt. the control flow diagram is the
%notaiton used in the diagram. For such programs, the termination property is
%easily derrivable\cite{complexity-of-loop-programs}. 

