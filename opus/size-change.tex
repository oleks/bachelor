\chapter{Size-Change Termination}

The size-change termination analysis builds upon the idea of flow analysis of
programs. In general, flow analysis aims to answer the question, ``What can we
say about a given point in a program without regard to the execution path taken
to that point?''. A ``point'' in a computer program, is in this case a primitive
operation such as an assignment, a condition branch, etc.

The idea is then to construct a graph where such points are nodes, and the arcs
in between them represent a transfer of control between the primitive
operations, that would otherwise occur under the execution of the program.
Such a node may have variable in-degree and out-degree. For instance, a
condition branch would usually have two possible transfers of control depending
on the outcome of the condition. Hence, it serves useful to label arcs
depending on when they are taken.

Such graphs are referred to as \emph{control flow graphs}.

With a control flow graph at hand, various optimization algorithms can be
devised to traverse the graph and deduce certain properties, such as
reoccurring primitive operations on otherwise static variables, see e.g.
\cite{kildall}.

\section{Control flow graphs in \D{}}

\subsection{Start and end nodes}

Every control flow graph has a start and an end node. These nodes do not
explicitly represent control primitives, but rather the start and end of a
program. \emph{A program cannot be started nor ended more than once}. The start
node is labelled $S$ and has out-degree $1$ and in-degree $0$. The end node is
labelled $E$ and has out-degree $0$, but variable in-degree, i.e. a program can
be ended in more than one way.

The control-flow graph for the empty program is hence:

\input{figures/cfg-empty}

\subsection{Function clauses}

While node construction and destruction are primitive operations in \D{}, we'll
refrain ourselves from delving into such details in the control flow graphs of
our programs. Indeed because \emph{node construction and destruction always
terminates}. Instead, we'll let a \emph{function clause} define a point in a
program.

The expression of the clause can thereafter make calls to its
enclosing\footnote{We say that a function \emph{consists} of function clauses
and a function clause is \emph{enclosed} in a function.}, or some other
function. Such calls are represented by transfer of control, that is, arcs.
Disregarding the cases where a function clause expression makes multiple calls
to the same function with different arguments, these arcs need not be
disjunctively labelled since all of these transitions happen unconditionally as
a result of evaluating the expression. More specifically, \emph{we consider the
order of evaluation to be insignificant}, and hence undeserving of labelling.
We further discuss the reasons for this below.

If calls are separated by node construction, the order in which those calls are
made is definitely insignificant. For instance, consider the expression
\mono{(f a).(g b)}, where \mono{f} and \mono{g} are some well-defined
functions, $\text{\mono{f}}\neq\text{\mono{g}}$, and \mono{a} and \mono{b} are
some bound variables. It makes no difference to the final result which of the
calls, \mono{f a} and \mono{g b}, is evaluated first. Indeed, they can be
evaluated in parallel, and we would still get the same result. This is easy to
see for any nested construction of results of function calls, as in e.g.
\mono{(f a).0.(g b)}.

On the other hand, the syntax and semantics of \D{} allow for function calls to
be nested as in e.g. the expression \mono{(f (g a) (h b))}, where \mono{h} is
also some well-defined function and is pairwise unequal to \mono{f} and
\mono{g}. While the order of evaluation of \mono{g a} and \mono{h b} is
\emph{insignificant} wrt. to one another, as with function calls separated by
construction, the order of evaluation of these two subexpressions wrt. to the
call to function \mono{f}, \emph{is significant to the result}, and
\emph{might} be significant to termination analysis in general. However, we'll
regard this as insignificant for the time being for mere simplicity. We'll come
back to the question of whether size-change termination analysis can benefit
from regarding this as significant later on.

We can now draw a control flow graph for the program define in
\referToListing{cfg-sample-1} as shown in \referToFigure{cfg-sample-1}.

\begin{lstlisting}[label=listing:cfg-sample-1,caption={A sample \D{} program, always returning \mono{0.0.0}.}]
f x y := x.y
g _ := 0
h _ := 0
i x y := (f ((h y).(g x)) (h y))
i input input
\end{lstlisting}

\input{figures/cfg-sample-1}

\subsection{Call cycles}

A call cycle occurs when there is a cyclical transition of control between the
nodes of a control flow graph. I.e. when there is a cycle in the control flow
graph.

\begin{lemma} We're concerned with call cycles in control flow graphs since
non-termination cannot occur if not for an infinite control flow cycle.
\end{lemma}

\begin{proof} If a program has a control flow graph with no cycles and does not
terminate, then one of the primitive operations, i.e. construction,
destruction, comparison or binding, does not terminate, which is certainly
absurd given the semantics of \D{}. \end{proof}

Call cycles in \D{} can occur in recursive or mutually recursive function
clauses.

We will henceforth refer to function clauses with recursive calls as
\emph{recursive clauses} and their counterparts, i.e. the base clauses of a
function declaration, \emph{terminal clauses}.

\subsection{Disregarding back-propagation}

It is worth noting that in \referToFigure{cfg-sample-1}, the clauses that make
no function calls have out-degree $0$. Technically, these functions \emph{do
transfer control} -- back to the callee. We may refer to this process as
\emph{back-propagation of control}. While considering back-propagation is
seemingly important to a concept that bases itself on the changes in the sizes
of the program values, we're only concerned with call cycles.

The thing with back-propagation is that forward-propagation after
back-propagation of a call cannot occur due to the way \D{} is defined. Hence,
what we are really concerned with is, ``how deep the rabbit hole goes'', before
we back-propagate, as back-propagation superimplies termination of the function
we're back-propagating out of.

\subsection{Dropping the start and end nodes}

The disregard of the back-propagation of control forces us to either redefine
the transition from the start node and the transitions to the end node. This is
because neither of these transitions are ever back-propagated, while all other
transitions \emph{must be} back-propagated if the program terminates.

Alternatively, disregard of back-propagation allows us to drop these nodes
completely and concentrate on the clauses and explicit calls within the clause
expressions. Hence, start and end nodes will not appear in any further graphs.

\subsection{Control flow graphs vs. abstract static call graphs}

Disregard of back-propagation allows us to consider control flow graphs
presented in this text as mere \emph{abstract static call graphs}, henceforth
referred to simply as, \emph{call graphs}. The abstraction applied to these
graphs compared to regular static call graphs is that the concrete arguments of
the function calls are not considered, and we merely consider how these values
can change in size from for a given function call. Interestingly, the problem
of termination analysis can be rephrased as the problem of determining whether
the regular static call graph of a program, i.e. the one containing the
concrete function arguments, is finite.

\subsection{Multiple calls to the same function}

Up until now we've only regarded expressions that don't make calls to the same
function with varying arguments. This is because these calls have to be
disjunctively labelled for the purposes of our analysis, because the use of
varying arguments \emph{may} mean varying decrease (or increase), in values for
the different calls within the expression. For this purpose we'll disjunctively
label \emph{all} the calls within an expression, if necessary, but remember
that this has nothing to do with evaluation order as has been discussed above.

This allows us to draw a control flow graph, or equivalently, a call graph,
for the program in \referToListing{cfg-sample-2}. Here, we've already
disjunctively labelled all of the calls in the expressions. This call graph is
drawn in \referToFigure{cfg-sample-2}.

\begin{lstlisting}[label=listing:cfg-sample-2,caption={A sample \D{} program, always returning \mono{(0.x).(0.y)}, where \mono{x} and \mono{y} are arbitrary \D{} values supplied by the user.}]
f x y := x.y
g x := 0.x
i x y := (0: f (1: g x) (2: g y))
i input input
\end{lstlisting}

\input{figures/cfg-sample-2}

\subsection{Multiple clauses}

If function clauses are nodes, and the function calls within the expressions of
the function clauses are unconditional transitions, what exactly happens if the
arguments supplied to the function clause fail to match the pattern declaration
for the clause?

The semantics of \D{} tell us to make an unconditional transition to the
immediately next clause of the function. There is at most one such transition
for any clause, and the last clause of a function declaration cannot fail to
pattern match\footnote{See \referToSection{d-sos}.}.

We'll refer to these transitions as \emph{fail transitions} and visually mark
them with a dotted line rather than a filled line. We need this way of visually
distinguishing fail transitions from the rest since they are conditionally
different, in that for any clause with a fail transition, either the fail
transition is chosen, or all the non-fail transitions are chosen
simultaneously.

Before we can draw the call graph we also need a way to distinguish the clauses
of a function wrt. the program text. We decide to enumerate the clauses
top-to-bottom starting with 0. Sometimes we'll annotate the program text with
these unique labels for each clause to make the call graph more readable.

Hence, we can now draw the call graph for the program defined in
\referToListing{cfg-loop} as shown in \referToFigure{cfg-loop}.

\begin{lstlisting}[label=listing:cfg-loop,caption={A simple, down-counting loop in \D{}.}]
f0: f 0 := 0
f1: f x._ := f x

f input
\end{lstlisting}

\input{figures/cfg-loop} 

For a more complex example, let's consider the call graph for the program
\mono{reverse} introduced in \referToSection{d-samples}. The program is
repeated in annotated form in \referToListing{cfg-reverse}, and its
corresponding call graph is shown in \referToFigure{cfg-reverse}.

\begin{lstlisting}[label=listing:cfg-reverse,caption={An annotated version of the program \mono{reverse} introduced in \referToSection{d-samples}.}]
r0: reverse 0 := 0
r1: reverse left.right := (0: reverse right).(1: reverse left)

reverse input
\end{lstlisting}

\input{figures/cfg-reverse} 

\subsection{Deeply nested function calls}

Blah

\section{Size-change termination principle}

Consider the program in \referToListing{cfg-loop} and it's corresponding call
graph in \referToFigure{cfg-loop}. Without any further information about the
control transitions, the program seemingly loops indefinitely. However, there
are some things that we can deduce about the control transitions.

\begin{lemma} If we can deduce for every control flow cycle in a prorgram that
it reduces a value of well-founded data-type on each iteration of the cycle,
then the value must eventually bottom out and the program must terminate.
\end{lemma}

\begin{proof} Assume for the sake of contradiction that a program that reduces
a value of a well-founded data type in each call cycle does not terminate.
Then, either the value reduces indefinitely, which is a contradiction to the
well-foundedness of its data type, or some noncyclic call sequence causes an
infinite loop, also an absurdity due to the definition of \D{}. \end{proof}

That is the \emph{size-change termination principle}. All values in \D{} are
inherently well-founded so what remains to be shown is how we can deduce from a
call cycle whether it reduces a value on each iteration.

\begin{lemma}\label{lemma:cycle-reduce} A control flow cycle reduces a value on
each iteration if at least one of the participating control transitions reduces
the value and all other control transitions do not increase that
value.\end{lemma}

\begin{proof} If a value is not reduced in a cycle, it either stays the same or
is increased. If it is increased, then at least one control transition must've
increased the value, an absurdity. If it stays the same then none of the
participating control transitions have neither increased nor decreased the
value, also an absurdity. \end{proof}

By the definition of call graphs, function clauses participate as nodes in a
call cycle. A control transition is a directed edge between two function
clauses where one clause is the \emph{source} and the other is the
\emph{target}. 

We can analyze how a value changes it's size through a call sequence by
analyzing the size relation between the variables bound in the source and the
variables bound in the target of every control transition.

\begin{definition} The relation $\Phi\ :\ C_{caller} \times C_{callee} \times
N_{caller} \times N_{callee} \rightarrow \{\bot,<,\leq\}$ is defined to be the
size relation between the caller and callee
clauses in \D{} where $N_{caller}$ are the names of the variables bound in the
caller, and $N_{callee}$ are the names of the variables bound in the callee.
Note, that we are only concerned with reductions and non-increases in size, all
other relationships are marked by the no relationship symbol $\bot$. Initially,
the relationship between all the clauses and their variables is $\bot$.
\end{definition}

The construction of the relationship $\Phi$ for a given transition depends
first and foremost on whether that transition is a fail or success transition.

\subsection{Fail transitions}

A fail transition occurs if the values passed to a given clause to match it's
pattern. If the values fail to match the pattern, no variables are bound and
hence no change in values can occur. The values are simply passed along as they
were to the next clause of the function declaration.

\begin{lemma} Fail transitions are transitive in the sense that the
relationship between the variables bound in the source and the variables bound
in the target is the same regardless of the number of fail transitions in
the path between the source and the target.\end{lemma}

\begin{proof} Follows from the semantics of \D{}. \end{proof}

We are not concerned with exact equivalence, hence all fail transitions in the
$\Phi$ relation return the relation $\leq$ for all variable pairs.

Note, that due to \D{} being first order and statically scoped, the variable
space is always initially empty when a function clause begins pattern matching.

\subsection{Success transitions}

Since \D{} is a call-by-value language, when a function call is encountered,
the source evaluates the arguments of the function call and generates some
\emph{values} before giving up control.

The values may hence be a nested construction of some concrete values, values
bound to variables in the source, and results of nested function calls. Without
further regard of nested function calls, this implies that a \emph{size
relation} can be deduced between the variables bound in the source and the
values that result from an evaluation of the function call arguments. 

Of course, we cannot deduce a precise size displacement as the values of the
bound variables may initially be \emph{unknown} at compile
time\footnote{Although some values can be deduced via static analysis of the
program, others can come in from the outside world via the 0-ary function
\mono{input} at run time.}.  However, we can deduce a \emph{safe} displacement
estimate, such that it is less than or equal to the actual displacement in
terms of absolute value. For instance, if the expression \mono{a.b} appears as
a function call argument, where \mono{a} and \mono{b} are some bound variables
with unknown values, and this argument evaluates to some value $v$, then we can
\emph{safely} say that $v>\text{\mono{a}}$, $v>\text{\mono{b}}$,
$v\geq\text{\mono{a.0}}$ and $v\geq\text{\mono{0.b}}$.

We decide to ignore the nested function calls because this would imply a more
complex static analysis of the program. Specifically, we're unable to say
anything about the result of the nested function call from the scope of the
source clause alone. Instead, we treat results from nested function calls
simply as variables with \emph{unknown} values. We also make sure to keep these
variables separate from the bound variables as there is no relationship to draw
between these ``variables'' and the variables bound in the
target\footnote{While this information may be useful for dead-code elimination
and other forms of static analysis, this is of little importance to size-change
termination.}.

More formally, given a function argument as the expression $x$, we construct
the expression $x^s$ where we replace all first-level nested function
calls\footnote{Nested function calls of nested function calls are hence
considered irrelevant to the derivation of the size relation of the top-level
call, however, they may become relevant as we derive the size relations of the
corresponding nested calls.} by auxiliary variables.  We group all those
auxiliary variables into the set of variable names $N_{calls}^s$ and all the
remaining variables into the set $N_{vars}^s$.  Furthermore we construct the
auxiliary variable names in such way that $N_{vars}^s\cup
N_{calls}^s=\emptyset$.  Hence, we obtain the tuple
$(x^c,N_{vars}^s,N_{calls}^s)$.

Continuing on with the example above, i.e. having the size relations
$\{v>\textt{a}, v> \textt{b}, v\geq \textt{a.0}, v\geq \textt{0.b}\}$, assume
that the target clause has the corresponding pattern \mono{x.y}. The question
henceforth is how do we draw the relationship that $\textt{a}\equiv\textt{x}$
and $\textt{b}\equiv\textt{y}$, or perhaps simply that the control transition
neither decreases nor increases any values. We can perform a corresponding
analysis on the pattern declaration and deduce the set of conditions that will
hold after pattern matching succeeds, indeed, $\{v>\textt{x}, v> \textt{y},
v\geq \textt{x.0}, v\geq \textt{0.y}\}$. The participation of $x$ in the same
kind of relations as $a$, and the participation of $y$ in the same kind of
relations as $b$, does not alone indicate their respective equivalence, since
the actual property that $v\equiv\textt{a.b}$ is lost.

On the other hand, if we had to formally define the relation that had to be
built between the variables bound in the source and the values that the
function call arguments evaluated to, this would be a relation between values
and some kind of ``abstract patterns'', as e.g. $v\geq\textt{0.b}$.

To simplify the entire process, instead of deducing actual size relations
between the variables bound in the source and the values that the function
arguments evaluate to, we can simply turn the function argument into the
abstract pattern to begin with. The actual size relations are hence withkept
and can be deduced at a later stage in the process.

Indeed, the tuple $(x^s,N_{vars}^s,N_{calls}^s)$ constitutes such an abstract
pattern already, since the expression $x^s$, contains no function calls and
hence syntactically matches a pattern in \D{}\footnote{See
\referToSection{d-syntax} if you're uncertain.}. We henceforth refer to such an
expression as $p^s$\footnote{Where $s$ stands for \emph{source}.}. Given a
clause with the pattern $p^t$\footnote{Where $t$ stands for \emph{target}.}, we
can easily deduce the set $N_{vars}^t$, which is the set of variable names used
in $p$. Our task is then to deduce a size relation between the variables in the
sets $N_{vars}^s$ and $N_{vars}^t$ given the tuples
$(p^s,N_{vars}^s,N_{calls}^s)$ and $(p^t,N_{vars}^t)$.

\subsection{Pattern matching}

Let the function $\phi\ :\ \mathbb{N} \times \mathbb{N} \rightarrow
\{<,\leq,\bot\}$ denote the function $\lambda N^t, N^s .
\Phi\left(C^t,C^s,N^t,N^s\right)$. In the following section we will discuss the
rules involved in deducing the function $\phi$, that is, the function $\Phi$
for some given source and target of a success transition.

For this purpose we will regard the tuples $(P^s,N_{vars}^s)$ and
$(P^t,N_{vars}^t)$, of a given success transition, where $P^s$ is the list of
abstract patterns derived from the function arguments in the source, and $P^t$
is the list of corresponding actual patterns in the target. Furthermore, let
$N_{vars}^s$ and $N_{vars}^t$ be unary functions of the type
$\mathbb{P}\rightarrow\mathbb{N}^*$, accepting a pattern and yielding the
variable names that are contained both in the input pattern and the sets
$N_{vars}^s$ and $N_{vars}^t$, respectively.

In the following analysis we will look at but one instance of the lists $P^s$
and $P^t$, namely the abstract pattern $p^s$ from the source and its
corresponding actual pattern in the declaration, $p^t$. In total, however, this
process has to be repeated for each such pair given the sets $P^s$ and $P^t$,
iteratively extending the definition of the relation $\phi$ to all variables
bound in the sets $N_{vars}^s$ and $N_{vars}^t$.

We initially define $\phi$ to yield the value $\bot$ for all arguments. We will
continuously modify this definition as we process $p^s$ and $p^t$. We denote
this within the semantics in a manner similar to the state $\sigma$ in the
semantics\footnote{See \referToSection{d-sos}.}. However, $\phi$ is now a
binary ``memory'', requiring both a target name and a source name (in that
order). For simplicity, we will borrow some suger coding from the matlab
notation which allows us to provide a collection in place of a single element
and let the runtime apply the given function to each element in the collection.
For instance, we might write that $\phi\left(N_{vars}^t(p^t), n^s\right)\mapsto
<$, meaning that all the target variables used in $p^t$ are strictly less than
the source variable $n^s$.

We now define a summoning rule, dividing the rules up into sub-rules:

\begin{equation}
{
    \left\langle\proc{A},p^t,p^s,\phi\right\rangle
    \rightarrow
    \phi'
  \vee
    \left\langle\proc{B},p^t,p^s,\phi\right\rangle
    \rightarrow
    \phi'
  \vee
    \left\langle\proc{C},p^t,p^s,\phi\right\rangle
    \rightarrow
    \phi'
  \vee
    \left\langle\proc{D},p^t,p^s,\phi\right\rangle
    \rightarrow
    \phi'
  \vee
    \left\langle\proc{E},p^t,p^s,\phi\right\rangle
    \rightarrow
    \phi'
}\over{
  \left\langle p^t,p^s,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

One of the simpler cases is when the abstract pattern $p^s$ is simply \mono{0},
or some name $n^s$, and $n^s\in N_{calls}^s$. Since no variables bound in the
source participate in $p^s$, then no relations need to be drawn to any of the
target variables that might appear in the corresponding $p^t$. Hence, $\phi$
need not be modified.

\begin{equation}\label{eq:sct-pattern-source-fail}
{
\left(
    p^s\rightarrow 0
  \vee
\left(
    p^s\rightarrow n^s
  \wedge
    n^s\notin N_{vars}^s
\right)
\right)
  \wedge
    \phi\rightarrow\phi'
}\over{
  \left\langle\proc{A},p^t,p^s,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

This has a symmetrical case. Indeed when $p^t$ is neither a destruction, nor
any name $n^t$, that is, it is \mono{\_} or \mono{0}. This pattern contains no
variables, and hence  no relations need to be drawn from any of the variables
that might appear in the corresponding $p^s$. Hence, $\phi$ need not be
modified in such a case either.

\begin{equation}
{
\left(
    p^t\rightarrow 0
\vee
    p^t\rightarrow \_
\right)
  \wedge
    \phi\rightarrow\phi'
}\over{
  \left\langle\proc{B},p^t,p^s,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

If $p^t$ is the name pattern $n^t$, the matters get a bit more complicated:

\begin{enumerate}

\item If $p^s$ is some node, then all the variables that occur in $p^s$, i.e.
$N_{vars}^s(p^s)$, will all be strictly less than $n^t$ by the semantics of
\D{}. However, we are not concerned with this relation, as we would like to
know when a value is decreased from source to target, and not, as in this case,
increased.

\item If $p^s$ is also some name pattern $n^s$, and  $n^s\in N^s_{vars}$, then
the values of these corresponding variables will be \emph{equivalent}. However,
we're not concerned with exact equivalence, and simply mark this relationship
with the weaker, but still sound relation, $\leq$:

\begin{equation}
{
    p^t\rightarrow n^t
  \wedge
    p^s\rightarrow n^s
  \wedge
    n^s\in N_{vars}^s
  \wedge
    \left\langle\phi\left(n^t, n^s\right)\mapsto \leq\right\rangle\rightarrow\phi'
}\over{
  \left\langle\proc{C},p^t,p^s,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

\end{enumerate}

If $p^t$ is a destruction and $p^s$ is the variable name $n^s$, then we can safely say that
all the variables that occur in $p^t$, i.e. $N_{vars}^t(p^t)$, are all strictly less
than the variable in $n^s$:

\begin{equation}
{
    p^t\rightarrow p^t_1\cdot p^t_2
  \wedge
    p^s\rightarrow n^s
  \wedge
    n^s\in N_{vars}^s
  \wedge
    \left\langle\phi\left(N_{vars}^t(p^t), n^s\right)\mapsto <\right\rangle\rightarrow\phi'
}\over{
  \left\langle\proc{D},p^t,p^s,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

If both $p^t$ and $p^s$ are a destructions, then the following recursive rule applies:

\begin{equation}
{
    p^t\rightarrow p^t_1\cdot p^t_2
  \wedge
    p^s\rightarrow p^s_1\cdot p^s_2
  \wedge
    \left\langle p^t_1, p^s_1, \phi\right\rangle
    \rightarrow
    \phi''
  \wedge
    \left\langle p^t_2, p^s_2, \phi''\right\rangle
    \rightarrow
    \phi'
}\over{
  \left\langle\proc{E},p^t,p^s,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

\section{Graph annotation}

Hence, we can deduce from \referToListing{cfg-loop}, that when $f_1$ makes a
call to $f_0$ it does so with a value strictly less then it's own argument,
i.e. the transition $f_1\rightarrow f_0$ strictly decreases a value. Visually
we will mark this with a $\downarrow$. The Lemmas \refer{lemma:d-pattern-leq}
and \refer{lemma:d-pattern-less} can be used to deduce the same sort of
relationship for the transitions $r_1\xrightarrow{0,1} r_0$ for
\referToListing{cfg-reverse}. These observations are summarised in
\referToFigure{sct-first}.

\input{figures/sct-first}

\subsection{Calls to multivariate functions}

The call graph notation used thus far has only been used for describing calls
to unary functions. As an example of a multivariate function, we may consider
the function \mono{normalized-less/2}, introduced in
\referToSection{d-size-less}. We use this function to define the program in
\referToListing{sct-normalized-less}. The corresponding call graph is shown in
\referToFigure{sct-normalized-less}.

\begin{lstlisting}[
  label={listing:sct-normalized-less},
  caption={A sample program with a multivariate function.}]
n0: normalized-less 0 b := b
n1: normalized-less _ 0 := 0
n2: normalized-less _.a _.b := normalized-less a b
normalized-less input input
\end{lstlisting}

\input{figures/sct-normalized-less}

The notation is straightforward, the juxtaposition of the $\downarrow$
indicates the size change of the respective arguments, read left to right as in
the function clause definition.

\subsection{Nonincreasing transitions}

There are cases where for a given transition in a call cycle, we can't tell
whether the sizes are strictly decreased or remain the same, but we can
definitely say that there is \emph{no increase} in the sizes of variables. As
an example, consider the program in \referToListing{sct-non-reducing}.

\begin{lstlisting}[
  label=listing:sct-non-reducing,
  caption={The binary function \mono{g} has a call cycle with nonincreasing
    sizes in variables.}]
g0: g 0 0 = 0
g1: g _.a b._ = g 0.a b.0
g input input
\end{lstlisting}


For the recursive clause $g_1$, it is unclear whether the sizes of the
variables are decreased in the transition $g_1\rightarrow g_0$, or not.
Specifically, if the arguments to \mono{g} are of the form \mono{0.\_} and
\mono{\_.0}, respectively, the size is \emph{not} decreased by the call. We'll
denote such transitions by the symbol $\Downarrow$. We can now draw the call
graph for the program in \referToListing{sct-non-reducing} as in
\referToFigure{sct-non-reducing}.

\input{figures/sct-non-reducing}

\subsection{Increasing transitions}


%Values are not always decreasing.

%Consider the following program:

\begin{lstlisting}[
  label=listing:sct-infinite-join,
  caption={The function \mono{infinite-join/2} infinitely joins..}]
f0: f a b = f a.b b.a
f input input
\end{lstlisting}

%The function clause expression constructs a new value by joining the two
%incoming arguments into a new node. While we're unable to say by how much the
%values are increased exactly, we know that both values are increased by at
%least 1 due to the creation of a node for each argument.


%\newpage

%A derrivable property from the program text is that the value sent as the
%argument in the transition $F_1\rightarrow F_0$ is \emph{always} strictly less
%than the value sent as the argument in the preceeding transition,
%$F_0\rightarrow F_1$. Additionally, we can universally state that \emph{false
%transitions from case nodes by definition can't change any values}. Hence, the
%infinite control flow sequence $F_0\rightarrow F_1\rightarrow F_0$ strictly
%decreases a well founded value in every iteration of the control flow cycle.
%This means that eventually the value bottoms out at $0$ which would validate
%the $F_0$ pattern and lead the program to terminate. In general, we can state
%the following:

%\begin{lemma}

%An infinite control flow sequnece infinitely decreases a well founded value if
%all the transitions in the corresponding control flow graph cycle either
%strictly decrease a value or keep all values unchanged, and at least one
%transition strictcly decreases a value.

%\end{lemma}

%\referToFigure{cfg-loop-down} showcases an edge-labelled control flow graph for
%\referToListing{d-loop}. Transitions for which we can \emph{safely} say that a
%value is decreased we mark with the symbol $\downarrow$, transitions for which
%we can \emph{safely} say that no value is increased we mark with the symbol
%$\top$. The graph is rather verbose and in future graphs we'll omit $\top$
%labels for nodes that universally can't increase a value, i.e. $S\rightarrow
%F_0$, $F_0\rightarrow E$ and $F_0\rightarrow F_1$.

%We can further prove the soundness of the size-change termination principle.

%\begin{proof} Assume for the sake of contradiction that there exists an
%infinite control flow sequence that infinitely decreases a well-founded data
%value. Assume furthermore that the program does not terminate. Then, some
%variable has to decrease indefinately, which contradicts with the definition of
%well-founded values. \end{proof}

%Unfortunately, this particular program can be trivially unfolded into a loop
%program, so the point of using size-change termination analysis may seem
%superflous. Indeed, the only change wrt. the control flow diagram is the
%notaiton used in the diagram. For such programs, the termination property is
%easily derrivable\cite{complexity-of-loop-programs}. 

