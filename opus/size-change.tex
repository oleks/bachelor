\chapter{Size-change termination}

The size-change termination analysis builds upon the idea of flow analysis of
programs. In general, flow analysis aims to answer the question, ``What can we
say about a given point in a program without regard to the execution path taken
to that point?''. A ``point'' in a computer program is in this case a primitive
operation such as an assignment, a condition branch, etc.

The idea is then to construct a graph where such points are nodes, and the arcs
in between them represent a transfer of control between the primitive
operations, that would otherwise occur under the execution of the program.
Such a node may have variable in-degree and out-degree from one given
primitive. For instance, a condition branch would usually have two possible
transfers of control depending on the outcome of the condition. Hence, it
serves useful to label arcs depending on when they are taken. The conditions
should clearly not overlap to avoid non-determination.

Such graphs are referred to as \emph{control flow graphs}.

With such a graph at hand, various optimization algorithms can be devised to
traverse the graph and deduce certain properties, such as reoccurring primitive
operations on otherwise static variables\cite{kildall}, etc.

\section{Control flow graphs in \D{}}

\subsection{Start and end nodes}

Every control flow graph has a start and an end node. These nodes do not
explicitly represent control primitives, but rather the start and end of a
program. \emph{A program cannot be started nor ended more than once}. The start
node is labelled $S$ and has out-degree $1$ and in-degree $0$. The end node is
labelled $E$ and has out-degree $0$, but variable in-degree, i.e. a program can
be ended in more than one way.

The control-flow graph for the empty program is hence:

\input{figures/cfg-empty}

\subsection{Function clauses}

While node construction and destruction are primitive operations in \D{}, we'll
refrain ourselves from delving into such details in the control flow graphs of
our programs. Indeed because \emph{node construction and destruction always
terminates}. Instead, we'll let a \emph{function clause} define a point in a
program.

The expression of the clause can thereafter make calls to its
enclosing\footnote{We say that a function \emph{consists} of function clauses
and a function clause is \emph{enclosed} in a function.}, or some other
function. Such calls are represented by transfer of control, that is, arcs.
Disregarding the cases where a function clause expression makes multiple calls
to the same function with different arguments, these arcs need not be
disjunctively labelled since all of these transitions happen unconditionally as
a result of evaluating the expression. More specifically, \emph{we consider the
order of evaluation to be insignificant}, and hence undeserving of labelling.
We further discuss the reasons for this below.

If calls are separated by node construction, the order in which those calls are
made is definitely insignificant. For instance, consider the expression
\mono{(f a).(g b)}, where \mono{f} and \mono{g} are some well-defined
functions, $\text{\mono{f}}\neq\text{\mono{g}}$, and \mono{a} and \mono{b} are
some bound variables. It makes no difference to the final result which of the
calls, \mono{f a} and \mono{g b}, is evaluated first. Indeed, they can be
evaluated in parallel, and we would still get the same result. This is easy to
see for any nested construction of results of function calls, as in e.g.
\mono{(f a).0.(g b)}.

On the other hand, the syntax and semantics of \D{} allow for function calls to
be nested as in e.g. the expression \mono{(f (g a) (h b))}, where \mono{h} is
also some well-defined function and is pairwise unequal to \mono{f} and
\mono{g}. While the order of evaluation of \mono{g a} and \mono{h b} is
\emph{insignificant} wrt. to one another, as with function calls separated by
construction, the order of evaluation of these two subexpressions wrt. to the
call to function \mono{f}, \emph{is significant to the result}, and
\emph{might} be significant to termination analysis in general. However, we'll
regard this as insignificant for the time being for mere simplicity. We'll come
back to the question of whether size-change termination analysis can benefit
from regarding this as significant later on.

We can now draw a control flow graph for the program define in
\referToListing{cfg-sample-1} as shown in \referToFigure{cfg-sample-1}.

\begin{lstlisting}[label=listing:cfg-sample-1,caption={A sample \D{} program, always returning \mono{0.0.0}.}]
f x y := x.y
g _ := 0
h _ := 0
i x y := (f ((h y).(g x)) (h y))
i input input
\end{lstlisting}

\input{figures/cfg-sample-1}

\subsection{Call cycles}

A call cycle occurs when there is a cyclical transition of control between the
nodes of a control flow graph. I.e. when there is a cycle in the control flow
graph.

\begin{lemma} We're concerned with call cycles in control flow graphs since
non-termination cannot occur if not for an infinite control flow cycle.
\end{lemma}

\begin{proof} If a program has a control flow graph with no cycles and does not
terminate, then one of the primitive operations, i.e. construction,
destruction, comparison or binding, does not terminate, which is certainly
absurd given the semantics of \D{}. \end{proof}

Call cycles in \D{} can occur in recursive or mutually recursive function
clauses.

We will henceforth refer to function clauses with recursive calls as
\emph{recursive clauses} and their counterparts, i.e. the base clauses of a
function declaration, \emph{terminal clauses}.

\subsection{Disregarding back-propagation}

It is worth noting that in \referToFigure{cfg-sample-1}, the clauses that make
no function calls have out-degree $0$. Technically, these functions \emph{do
transfer control} -- back to the callee. We may refer to this process as
\emph{back-propagation of control}. While considering back-propagation is
seemingly important to a concept that bases itself on the changes in the sizes
of the program values, we're only concerned with call cycles.

The thing with back-propagation is that forward-propagation after
back-propagation of a call cannot occur due to the way \D{} is defined. Hence,
what we are really concerned with is, ``how deep the rabbit hole goes'', before
we back-propagate, as back-propagation superimplies termination of the function
we're back-propagating out of.

\subsection{Dropping the start and end nodes}

The disregard of the back-propagation of control forces us to either redefine
the transition from the start node and the transitions to the end node. This is
because neither of these transitions are ever back-propagated, while all other
transitions \emph{must be} back-propagated if the program terminates.

Alternatively, disregard of back-propagation allows us to drop these nodes
completely and concentrate on the clauses and explicit calls within the clause
expressions. Hence, start and end nodes will not appear in any further graphs.

\subsection{Control flow graphs vs. abstract static call graphs}

Disregard of back-propagation allows us to consider control flow graphs
presented in this text as mere \emph{abstract static call graphs}, henceforth
referred to simply as, \emph{call graphs}. The abstraction applied to these
graphs compared to regular static call graphs is that the concrete arguments of
the function calls are not considered, and we merely consider how these values
can change in size from for a given function call. Interestingly, the problem
of termination analysis can be rephrased as the problem of determining whether
the regular static call graph of a program, i.e. the one containing the
concrete function arguments, is finite.

\subsection{Multiple calls to the same function}

Up until now we've only regarded expressions that don't make calls to the same
function with varying arguments. This is because these calls have to be
disjunctively labelled for the purposes of our analysis, because the use of
varying arguments \emph{may} mean varying decrease (or increase), in values for
the different calls within the expression. For this purpose we'll disjunctively
label \emph{all} the calls within an expression, if necessary, but remember
that this has nothing to do with evaluation order as has been discussed above.

This allows us to draw a control flow graph, or equivalently, a call graph,
for the program in \referToListing{cfg-sample-2}. Here, we've already
disjunctively labelled all of the calls in the expressions. This call graph is
drawn in \referToFigure{cfg-sample-2}.

\begin{lstlisting}[label=listing:cfg-sample-2,caption={A sample \D{} program, always returning \mono{(0.x).(0.y)}, where \mono{x} and \mono{y} are arbitrary \D{} values supplied by the user.}]
f x y := x.y
g x := 0.x
i x y := (0: f (1: g x) (2: g y))
i input input
\end{lstlisting}

\input{figures/cfg-sample-2}

\subsection{Multiple clauses}

If function clauses are nodes, and the function calls within the expressions of
the function clauses are unconditional transitions, what exactly happens if the
arguments supplied to the function clause fail to match the pattern declaration
for the clause?

The semantics of \D{} tell us to make an unconditional transition to the
immediately next clause of the function. There is at most one such transition
for any clause, and the last clause of a function declaration cannot fail to
pattern match\footnote{See \referToSection{d-sos}.}.

We'll refer to these transitions as \emph{fail transitions} and visually mark
them with a dotted line rather than a filled line. We need this way of visually
distinguishing fail transitions from the rest since they are conditionally
different, in that for any clause with a fail transition, either the fail
transition is chosen, or all the non-fail transitions are chosen
simultaneously.

Before we can draw the call graph we also need a way to distinguish the clauses
of a function wrt. the program text. We decide to enumerate the clauses
top-to-bottom starting with 0. Sometimes we'll annotate the program text with
these unique labels for each clause to make the call graph more readable.

Hence, we can now draw the call graph for the program defined in
\referToListing{cfg-loop} as shown in \referToFigure{cfg-loop}.

\begin{lstlisting}[label=listing:cfg-loop,caption={A simple, down-counting loop in \D{}.}]
f0: f 0 := 0
f1: f x._ := f x

f input
\end{lstlisting}

\input{figures/cfg-loop} 

For a more complex example, let's consider the call graph for the program
\mono{reverse} introduced in \referToSection{d-samples}. The program is
repeated in annotated form in \referToListing{cfg-reverse}, and its
corresponding call graph is shown in \referToFigure{cfg-reverse}.

\begin{lstlisting}[label=listing:cfg-reverse,caption={An annotated version of the program \mono{reverse} introduced in \referToSection{d-samples}.}]
r0: reverse 0 := 0
r1: reverse left.right := (0: reverse right).(1: reverse left)

reverse input
\end{lstlisting}

\input{figures/cfg-reverse} 

\subsection{Deeply nested function calls}

Blah

\section{Size-change termination principle}

Consider the program in \referToListing{cfg-loop} and it's corresponding call
graph in \referToFigure{cfg-loop}. Without any further information about the
control transitions, the program seemingly loops indefinitely. However, there
are some things that we can deduce about the control transitions.

\begin{lemma} If we can deduce for every control flow cycle in a prorgram that
it reduces a value of well-founded data-type on each iteration of the cycle,
then the value must eventually bottom out and the program must terminate.
\end{lemma}

\begin{proof} Assume for the sake of contradiction that a program that reduces
a value of a well-founded data type in each call cycle does not terminate.
Then, either the value reduces indefinitely, which is a contradiction to the
well-foundedness of its data type, or some noncyclic call sequence causes an
infinite loop, also an absurdity due to the definition of \D{}. \end{proof}

That is the \emph{size-change termination principle}. All values in \D{} are
inherently well-founded so what remains to be shown is how we can deduce from a
call cycle whether it reduces a value on each iteration.

\begin{lemma}\label{lemma:cycle-reduce} A control flow cycle reduces a value on
each iteration if at least one of the participating control transitions reduces
the value and all other control transitions do not increase that
value.\end{lemma}

\begin{proof} If a value is not reduced in a cycle, it either stays the same or
is increased. If it is increased, then at least one control transition must've
increased the value, an absurdity. If it stays the same then none of the
participating control transitions have neither increased nor decreased the
value, also an absurdity. \end{proof}

By the definition of call graphs, function clauses participate as nodes in a
call cycle. A control transition is a directed edge between two function
clauses where one clause is the \emph{source} and the other is the
\emph{destination}. 

We can analyze how a value changes it's size through a call sequence by
analyzing the size relation between the variables bound in the source and the
variables bound in the destination of every control transition.

%how the variables bound in the source relate to the variables bound in the
%destination.

\begin{definition} The relation $\Upsilon\ :\ C_{caller} \times N_{caller}
\times C_{callee} \times N_{callee} \rightarrow \{\bot,<,\leq\}$ is defined to
be the size relation between the caller ($C_{caller}$) and callee
($C_{callee}$) clauses in \D{} where $N_{caller}$ are the names of the
variables bound in the caller, and $N_{callee}$ are the names of the variables
bound in the callee. Note, that we are only concerned with reductions and
non-increases in size, all other relationships are marked by the no
relationship symbol $\bot$. Initially, the relationship between all the clauses
and their variables is $\bot$. \end{definition}

The construction of the relationship $\Upsilon$ for a given transition depends
first and foremost on whether that transition is a fail or success transition.

\subsection{Fail transitions}

A fail transition occurs if the values passed to a given clause to match it's
pattern. If the values fail to match the pattern, no variables are bound and
hence no change in values can occur. The values are simply passed along as they
were to the next clause of the function declaration.

\begin{lemma} Fail transitions are transitive in the sense that the
relationship between the variables bound in the source and the variables bound
in the destination is the same regardless of the number of fail transitions in
the path between the source and the destination.\end{lemma}

\begin{proof} Follows from the semantics of \D{}. \end{proof}

Hence all fail transitions in the $\Upsilon$ relation return the result $\leq$
for all variable pairs.

Note, that due to \D{} being first order and statically scoped, the variable
space is always initially empty when a function clause begins pattern matching.

\subsection{Success transitions}

Since \D{} is a call-by-value language, when a function call is encountered,
the source evaluates the arguments of the function call and generates some
\emph{values} before giving up control.

The values may hence be a nested construction of some concrete values, values
bound to variables in the source, and results of nested function calls. Without
further regard of nested function calls, this implies that a \emph{size
relation} can be deduced between the variables bound in the source and the
values that result from an evaluation of the function call arguments. 

Of course, we cannot deduce a precise size displacement as the values of the
bound variables may initially be \emph{unknown} at compile
time\footnote{Although some values can be deduced via static analysis of the
program, others can come in from the outside world via the 0-ary function
\mono{input} at run time.}.  However, we can deduce a \emph{safe} displacement
estimate, such that it is less than or equal to the actual displacement in
terms of absolute value. For instance, if the expression \mono{a.b} appears as
a function call argument, where \mono{a} and \mono{b} are some bound variables
with unknown values, and this argument evaluates to some value $v$, then we can
\emph{safely} say that $v>\text{\mono{a}}$, $v>\text{\mono{b}}$,
$v\geq\text{\mono{a.0}}$ and $v\geq\text{\mono{0.b}}$.

We decide to ignore the nested function calls because this would imply a more
complex static analysis of the program. Specifically, we're unable to say
anything about the result of the nested function call from the scope of the
source clause alone. Instead, we treat results from nested function calls
simply as variables with \emph{unknown} values. We also make sure to keep these
variables separate from the bound variables as there is no relationship to draw
between these ``variables'' and the variables bound in the
destination\footnote{While this information may be useful for dead-code
elimination and other forms of static analysis, this is of little importance to
size-change termination.}.

More formally, given a function argument as the expression $x$, we construct
the expression $x^c$ where we replace all first-level nested function
calls\footnote{Nested function calls of nested function calls are hence
considered irrelevant to the derivation of the size relation of the top-level
call, however, they may become relevant as we derive the size relations of the
corresponding nested calls.} by auxiliary variables.  We group all those
auxiliary variables into the set of variable names $N_{calls}^c$ and all the
remaining variables into the set $N_{vars}^c$.  Furthermore we construct the
auxiliary variable names in such way that $N_{vars}^c\cup
N_{calls}^c=\emptyset$.  Hence, we obtain the tuple
$(x^c,N_{vars}^c,N_{calls}^c)$.

Continuing on with the example above, i.e. having the size relations
$\{v>\textt{a}, v> \textt{b}, v\geq \textt{a.0}, v\geq \textt{0.b}\}$, assume
that the destination clause has the corresponding pattern \mono{x.y}. The
question henceforth is how do we draw the relationship that
$\textt{a}\equiv\textt{x}$ and $\textt{b}\equiv\textt{y}$, or perhaps simply
that the control transition neither decreases nor increases any values. We can
perform a corresponding analysis on the pattern declaration and deduce the set
of conditions that will hold after pattern matching succeeds, indeed,
$\{v>\textt{x}, v> \textt{y}, v\geq \textt{x.0}, v\geq \textt{0.y}\}$. The
participation of $x$ in the same kind of relations as $a$, and the
participation of $y$ in the same kind of relations as $b$, does not alone
indicate their respective equivalence, since the actual property that
$v\equiv\textt{a.b}$ is lost.

On the other hand, if we had to formally define the relation that had to be
built between the variables bound in the source and the values that the
function call arguments evaluated to, this would be a relation between values
and some kind of ``abstract patterns'', as e.g. $v\geq\textt{0.b}$.

To simplify the entire process, instead of deducing actual size relations
between the variables bound in the source and the values that the function
arguments evaluate to, we can simply turn the function argument into the
abstract pattern to begin with. The actual size relations are hence withkept
and can be deduced at a later stage in the process.

Indeed, the tuple $(x^c,N_{vars}^c,N_{calls}^c)$ constitutes such an abstract
pattern already, since the expression $x^c$, contains no function calls and
hence syntactically matches a pattern in \D{}\footnote{See
\referToSection{d-syntax} if you're uncertain.}. Given a clause with the
pattern $p$, we can easily deduce the set $N_{vars}^p$, which is the set of
variable names used in $p$. Our task is then to deduce a size relation between
the variables in the sets $N_{vars}^c$ and $N_{vars}^p$ given the tuples
$(x^c,N_{vars}^c,N_{calls}^c)$ and $(p,N_{vars}^p)$. 

\subsection{Pattern matching}

Let the function $\phi\ :\ N \times N \rightarrow \{<,\leq,\bot\}$ denote the
function $\lambda N^c, N^p . \Phi\left(C^c,N^c,C^p,N^p\right)$. We will in the
following section discuss the rules involved in deducing the function $\phi$,
that is, the function $\Phi$ for some given source and destination function
clauses.

In the following rules we assume that $N_{vars}^c$ and $N_{vars}^p$ are always
available. We don't pass them around since theses sets need not be modified in
any of the rules. We use matlab-like set notation, i.e. providing a set where a
scalar is expected yields an execution of the function for each element in the
set.

The simplest case is when the abstract pattern $x$ is simply \mono{0}, in this
case, regardless of the clause pattern $p$, no relation needs to be drawn
between the sets $N_{vars}^c$ and $N_{vars}^p$, as the following rule states:

\begin{equation}
{
    x\rightarrow 0
  \wedge
    \phi\rightarrow\phi'
}\over{
  \left\langle\proc{B},p,x,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

Likewise, clause patterns that are not destructions nor name patterns, have no
effect on the relationship between the sets $N_{vars}^c$ and $N_{vars}^p$
either:

\begin{equation}
{
\left(
    p\rightarrow \_
\vee
    p\rightarrow 0
\right)
  \wedge
    \phi\rightarrow\phi'
}\over{
  \left\langle\proc{D},p,x,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

Name patterns in clauses have a more complex effect. First, if the expression
$x$ is some node, then all the variables that occur in $x$, i.e.
$N_{vars}^c(x)$, are all strictly less than $n$.

\begin{equation}
{
    p\rightarrow n
  \wedge
    x\rightarrow x'\cdot x''
  \wedge
    \left\langle\phi\left(N_{vars}^c(x), n\right)\mapsto <\right\rangle\rightarrow\phi'
}\over{
  \left\langle\proc{A},p,x,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

Second, if the expression $x$ is just a variable reference, as is the pattern
$p$, then the values of these variables will be equivalent. However, we're not
concerned with exact equivalence and simply mark this relationship with the
weaker relation, $\leq$:

\begin{equation}
{
    p\rightarrow n^p
  \wedge
    x\rightarrow n^c
  \wedge
    \left\langle\phi\left(n^c, n^p\right)\mapsto \leq\right\rangle\rightarrow\phi'
}\over{
  \left\langle\proc{C},p,x,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}



Patterns with destructions are evaluated recursively as follows:

\begin{equation}
{
    p\rightarrow p'\cdot p''
  \wedge
    x\rightarrow x'\cdot x''
  \wedge
    \left\langle p', x', \phi\right\rangle
    \rightarrow
    \phi''
  \wedge
    \left\langle p'', x'', \phi''\right\rangle
    \rightarrow
    \phi'
}\over{
  \left\langle\proc{E},p,x,\phi\right\rangle
  \rightarrow
  \phi'
}
\end{equation}

For instance, consider the more complex program \mono{reverse} in
\referToListing{cfg-reverse} and it's corresponding control flow graph in
\referToFigure{cfg-reverse}. The abstract pattern for the success transition 0
is simply \mono{right} with the set $N_{vars}'$ consisting merely of the
variable \mono{right} with an unknown value. The abstract pattern is matched to
the actual pattern \mono{left.right}, and for the sake of avoiding ambiguity
let us refer to that pattern as simply \mono{a.b}. Hence, the variable is
destructed into two children, \mono{a} and \mono{b} such that
$\textt{a}<\textt{right}$ and $\textt{b}<\textt{right}$.


\begin{definition} The relation $\Upsilon\ :\ C_{caller} \times N_{caller} \times C_{callee} \times B_{callee}\rightarrow
\{>,\}$, between the values of the bound variables of the
function clause and the values that the arguments of the function call evaluate
to. $\uparrow$ indicates an increase, $\Uparrow$ a nondecrease.\end{definition}

\begin{lemma}

\end{lemma}
 
This is followed by the source transmitting the resulting values to the
destination, i.e.  performing a control transition and the destination pattern
matching the incoming values, the result of which may be binding of variables
in the destination clause.

Variables (or names) are bound to values, hence the relation $\nabla$ is a
injective relation between variable names, where each mapping is annotated with
the change in size indicating a lower bound on the difference between the
values of the variables.

In the following sections we discuss how we can deduce the lower bound on the
difference between the values of the variables.


\subsection{Pattern matching}

\begin{definition} For the sake of further discussion we'll regard the
underscore pattern simply as a uniquely named variable that is not used in the
expression of the function clause, i.e. it is also bound to a
value.\end{definition}

\begin{lemma}\label{lemma:d-pattern-leq} If the single pattern declaration
includes a variable name, we can safely state that if some variable is bound as
a result of pattern matching the corresponding argument, the value that
variable is bound to is less than or equal to the value of the original
argument. \end{lemma}

\begin{proof} There is no construction operator for a pattern
declaration.\end{proof}

\begin{lemma}\label{lemma:d-pattern-less} If a destruction operator
participates in a pattern declaration, then any variable bound as a result of
pattern matching the corresponding argument will be bound to a value strictly
less than the value of the original argument.\end{lemma}

\begin{proof} A variable can be bound to the actual value of the argument iff
the entire pattern is just a variable name. If the pattern contains at least
one destruction operator, the syntax rules of \D{} cause for that operation to
be performed first when pattern matching the argument. Hence, any value that
can be bound thereafter has a value with at least one node less than the
original value of the argument. \end{proof}

Hence, we can deduce from \referToListing{cfg-loop}, that when $f_1$ makes a
call to $f_0$ it does so with a value strictly less then it's own argument,
i.e. the transition $f_1\rightarrow f_0$ strictly decreases a value. Visually
we will mark this with a $\downarrow$. The Lemmas \refer{lemma:d-pattern-leq}
and \refer{lemma:d-pattern-less} can be used to deduce the same sort of
relationship for the transitions $r_1\xrightarrow{0,1} r_0$ for
\referToListing{cfg-reverse}. These observations are summarised in
\referToFigure{sct-first}.

\input{figures/sct-first}

\subsection{Calls to multivariate functions}

The call graph notation used thus far has only been used for describing calls
to unary functions. As an example of a multivariate function, we may consider
the function \mono{normalized-less/2}, introduced in
\referToSection{d-size-less}. We use this function to define the program in
\referToListing{sct-normalized-less}. The corresponding call graph is shown in
\referToFigure{sct-normalized-less}.

\begin{lstlisting}[
  label={listing:sct-normalized-less},
  caption={A sample program with a multivariate function.}]
n0: normalized-less 0 b := b
n1: normalized-less _ 0 := 0
n2: normalized-less _.a _.b := normalized-less a b
normalized-less input input
\end{lstlisting}

\input{figures/sct-normalized-less}

The notation is straightforward, the juxtaposition of the $\downarrow$
indicates the size change of the respective arguments, read left to right as in
the function clause definition.

\subsection{Nonincreasing transitions}

There are cases where for a given transition in a call cycle, we can't tell
whether the sizes are strictly decreased or remain the same, but we can
definitely say that there is \emph{no increase} in the sizes of variables. As
an example, consider the program in \referToListing{sct-non-reducing}.

\begin{lstlisting}[
  label=listing:sct-non-reducing,
  caption={The binary function \mono{g} has a call cycle with nonincreasing
    sizes in variables.}]
g0: g 0 0 = 0
g1: g _.a b._ = g 0.a b.0
g input input
\end{lstlisting}


For the recursive clause $g_1$, it is unclear whether the sizes of the
variables are decreased in the transition $g_1\rightarrow g_0$, or not.
Specifically, if the arguments to \mono{g} are of the form \mono{0.\_} and
\mono{\_.0}, respectively, the size is \emph{not} decreased by the call. We'll
denote such transitions by the symbol $\Downarrow$. We can now draw the call
graph for the program in \referToListing{sct-non-reducing} as in
\referToFigure{sct-non-reducing}.

\input{figures/sct-non-reducing}

\subsection{Increasing transitions}


%Values are not always decreasing.

%Consider the following program:

\begin{lstlisting}[
  label=listing:sct-infinite-join,
  caption={The function \mono{infinite-join/2} infinitely joins..}]
f0: f a b = f a.b b.a
f input input
\end{lstlisting}

%The function clause expression constructs a new value by joining the two
%incoming arguments into a new node. While we're unable to say by how much the
%values are increased exactly, we know that both values are increased by at
%least 1 due to the creation of a node for each argument.


%\newpage

%A derrivable property from the program text is that the value sent as the
%argument in the transition $F_1\rightarrow F_0$ is \emph{always} strictly less
%than the value sent as the argument in the preceeding transition,
%$F_0\rightarrow F_1$. Additionally, we can universally state that \emph{false
%transitions from case nodes by definition can't change any values}. Hence, the
%infinite control flow sequence $F_0\rightarrow F_1\rightarrow F_0$ strictly
%decreases a well founded value in every iteration of the control flow cycle.
%This means that eventually the value bottoms out at $0$ which would validate
%the $F_0$ pattern and lead the program to terminate. In general, we can state
%the following:

%\begin{lemma}

%An infinite control flow sequnece infinitely decreases a well founded value if
%all the transitions in the corresponding control flow graph cycle either
%strictly decrease a value or keep all values unchanged, and at least one
%transition strictcly decreases a value.

%\end{lemma}

%\referToFigure{cfg-loop-down} showcases an edge-labelled control flow graph for
%\referToListing{d-loop}. Transitions for which we can \emph{safely} say that a
%value is decreased we mark with the symbol $\downarrow$, transitions for which
%we can \emph{safely} say that no value is increased we mark with the symbol
%$\top$. The graph is rather verbose and in future graphs we'll omit $\top$
%labels for nodes that universally can't increase a value, i.e. $S\rightarrow
%F_0$, $F_0\rightarrow E$ and $F_0\rightarrow F_1$.

%We can further prove the soundness of the size-change termination principle.

%\begin{proof} Assume for the sake of contradiction that there exists an
%infinite control flow sequence that infinitely decreases a well-founded data
%value. Assume furthermore that the program does not terminate. Then, some
%variable has to decrease indefinately, which contradicts with the definition of
%well-founded values. \end{proof}

%Unfortunately, this particular program can be trivially unfolded into a loop
%program, so the point of using size-change termination analysis may seem
%superflous. Indeed, the only change wrt. the control flow diagram is the
%notaiton used in the diagram. For such programs, the termination property is
%easily derrivable\cite{complexity-of-loop-programs}. 

